import pandas as pd
import numpy as np
from typing import List, Dict, Tuple
from sentence_transformers import SentenceTransformer
from tqdm import tqdm
from sklearn.cluster import DBSCAN
from sklearn.metrics.pairwise import cosine_distances

def load_and_preprocess_data(file_path: str) -> pd.DataFrame:
    """
    –û–±—ä–µ–¥–∏–Ω—è–µ—Ç –∫–æ–ª–æ–Ω–∫–∏ –Ω–∞–∑–≤–∞–Ω–∏–µ –∏ –æ–ø–∏—Å–∞–Ω–∏–µ –≤ –æ–¥–Ω—É + –∑–∞–ø–æ–ª–Ω—è–µ—Ç –ø—Ä–æ–ø—É—Å–∫–∏
    ['idea_id', 'full_text']
    """
    df = pd.read_csv(file_path, sep=';', dtype=str)
    df.fillna('', inplace=True)

    df['full_text'] = df['–ù–∞–∑–≤–∞–Ω–∏–µ'].str.strip() + '. ' + df['–û–ø–∏—Å–∞–Ω–∏–µ'].str.strip()
    df['full_text'] = df['full_text'].str.strip().str.replace(r'\s+', ' ', regex=True)

    df = df[df['full_text'].str.strip() != '']
    df.rename(columns={'–ù–æ–º–µ—Ä –∏–¥–µ–∏': 'idea_id'}, inplace=True)
    df = df[['idea_id', 'full_text']].reset_index(drop=True)

    return df

def compute_embeddings(texts: List[str], model_name: str = 'all-MiniLM-L6-v2') -> np.ndarray:
    """
    –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è —Å–ø–∏—Å–∫–∞ –∏–∑ —Å—Ç—Ä–æ–∫ –≤ full_text
    """
    model = SentenceTransformer(model_name)
    embeddings = model.encode(texts, show_progress_bar=True, convert_to_numpy=True)
    return embeddings

def save_embeddings(df: pd.DataFrame, embeddings: np.ndarray, output_path: str) -> None:
    """
    –°–æ—Ö—Ä–∞–Ω—è–µ—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –≤ CSV: idea_id + –≤–µ–∫—Ç–æ—Ä —ç–º–±–µ–¥–¥–∏–Ω–≥–∞.
    """
    df_copy = df[['idea_id']].copy()
    df_copy['embedding'] = [emb.tolist() for emb in embeddings]
    df_copy.to_json(output_path, orient='records', lines=True, force_ascii=False)

def load_embeddings(path: str) -> Tuple[List[str], np.ndarray]:
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏–¥–µ–∏ –∏ –∏—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –∏–∑ JSONL.
    """
    records = pd.read_json(path, lines=True)
    idea_ids = records['idea_id'].tolist()
    embeddings = np.vstack(records['embedding'].values)
    return idea_ids, embeddings

def start_p_1():
    print("–®–∞–≥ 1/3: –ó–∞–≥—Ä—É–∂–∞–µ–º –∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ...")
    df = load_and_preprocess_data('data.csv')
    
    print("–®–∞–≥ 2/3: –°—Ç—Ä–æ–∏–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏...")
    texts = df['full_text'].tolist()
    texts = list(tqdm(texts, desc="üìÑ –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤", unit="—Ç–µ–∫—Å—Ç"))
    embeddings = compute_embeddings(texts)
    
    print("–®–∞–≥ 3/3: –°–æ—Ö—Ä–∞–Ω—è–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –≤ —Ñ–∞–π–ª...")
    save_embeddings(df, embeddings, 'embeddings.jsonl')
    
    print("–≠–º–±–µ–¥–¥–∏–Ω–≥–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ 'embeddings.jsonl'")

def cluster_embeddings(
    idea_ids: List[str],
    embeddings: np.ndarray,
    eps: float = 0.25,
    min_samples: int = 2
) -> pd.DataFrame:
    """
    –ö–ª–∞—Å—Ç–µ—Ä–∏–∑—É–µ—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —Å –ø–æ–º–æ—â—å—é DBSCAN –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç DataFrame —Å –º–µ—Ç–∫–∞–º–∏ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤.
    """
    print("–í—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ (DBSCAN)...")
    clustering = DBSCAN(metric='cosine', eps=eps, min_samples=min_samples)
    labels = clustering.fit_predict(embeddings)

    df = pd.DataFrame({
        'idea_id': idea_ids,
        'cluster_id': labels
    })

    print("–ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
    return df

def extract_duplicates_and_uniques(
    df_clusters: pd.DataFrame
) -> Tuple[List[List[str]], List[str]]:
    """
    –î–µ–ª–∏—Ç –∏–¥–µ–∏ –Ω–∞ –∫–ª–∞—Å—Ç–µ—Ä—ã –ø–æ–≤—Ç–æ—Ä–æ–≤ –∏ —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ.
    """
    print("–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–ª–∞—Å—Ç–µ—Ä—ã –Ω–∞ –ø–æ–≤—Ç–æ—Ä—ã –∏ —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∏–¥–µ–∏...")

    groups = df_clusters.groupby('cluster_id')['idea_id'].apply(list)

    duplicates = []
    for cid, group in tqdm(groups.items(), desc="–ü–æ–∏—Å–∫ –ø–æ–≤—Ç–æ—Ä–æ–≤", unit="–∫–ª–∞—Å—Ç–µ—Ä"):
        if cid != -1 and len(group) > 1:
            duplicates.append(group)

    uniques = groups.get(-1, [])

    print(f"–ü–æ–≤—Ç–æ—Ä—è—é—â–∏—Ö—Å—è –≥—Ä—É–ø–ø: {len(duplicates)}")
    print(f"–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∏–¥–µ–π: {len(uniques)}")

    return duplicates, uniques

def start_p_2():
    idea_ids, embeddings = load_embeddings('embeddings.jsonl')
    df_clusters = cluster_embeddings(idea_ids, embeddings, eps=0.25, min_samples=2)
    duplicate_groups, unique_ideas = extract_duplicates_and_uniques(df_clusters)
    """
    print("–ù–∞–π–¥–µ–Ω–æ –ø–æ–≤—Ç–æ—Ä—è—é—â–∏—Ö—Å—è –≥—Ä—É–ø–ø:", len(duplicate_groups))
    for i, group in enumerate(duplicate_groups, 1):
        print(f"–ì—Ä—É–ø–ø–∞ {i}: {group}")

    print("–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∏–¥–µ–π:", len(unique_ideas))
    """
    
start_p_2()
